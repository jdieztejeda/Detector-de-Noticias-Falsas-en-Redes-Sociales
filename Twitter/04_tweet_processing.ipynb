{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Process Tweets for Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective: Process Tweets for text analysis including topic modelling and wordcloud visualization. Processing includes separating words within hashtags, removing entity-specific words, stop words, and lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous step we obtained all tweets for our sample of colleges and universities. Now, however, we must process the messy tweet text for analysis. There are many different approaches to cleaning tweet data for text analysis and topic modelling. For example, some authors combine Tweets with the same hashtag into a single document. Upon inspection, however,  I decided that would not be helpful in the context of college and university's social media use. This is because many universities use hashtags that are just that college's name (too generic and frequent) e.g. #somecollege or specific university hashtags that are not used frequently (e.g. #somecollegehomecoming). \n",
    "\n",
    "Additionally, some colleges use only a hashtag when mentioning the topic I am most interested in for this work -- #diversity, while others do not use a hashtag, e.g. \"We support #womenoncampus\" vs \"We support women on campus.\" Because of this, I use the `wordsegment` package to attempt to split multi-word hashtags. Note however that this is not a perfect process when words overlap. \n",
    "\n",
    "Finally, because this analysis is used in words used across colleges, not just at a particular college, I remove words that  are only used by one college in the data. This helps with the problem mentioned above where colleges hashtag their own names. Their own names will occur frequently in the data, but not frequently enough to be removed by standard text processing techniques, such as removing words that occur in over a certain percentage of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `preprocessor` package will remove emojis, mentions, and URLs from tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: There is a bug installing this package in windows that was fixed by a kind user on \n",
    "# Github. Windows users should download as follows\n",
    "# pip install git+git://github.com/iamRusty/preprocessor\n",
    "import preprocessor as pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, glob, datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `wordsegment` package splits multiword hashtags into individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordsegment import load, segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Help with memory issues\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `unicodedata` package helps to convert unicode to ascii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nltk` and `gensim` packages are used for stemming and removing stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laure\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# Tools for LDA\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\laure\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\laure\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to Clean the Tweet Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data \n",
    "twitter_files = glob.glob('../data/twitter_data/*.json')\n",
    "test_file = twitter_files[1]\n",
    "test_df = pd.read_json(test_file, encoding='utf-8', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#UAHDiscoveryDay2017 Group Silver: Your first session is the Welcome! Join us in the Charger Union Theater for a quick welcome and rundown of your day on campus.\n"
     ]
    }
   ],
   "source": [
    "# Test tweet\n",
    "test_tweet = test_df.tweet[99]\n",
    "print(test_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load segmentation dictionary\n",
    "load()\n",
    "# Set options for Twitter processing - remove url, mention, and emojis\n",
    "pre.set_options(pre.OPT.URL, pre.OPT.EMOJI, pre.OPT.MENTION, pre.OPT.SMILEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will clean the hashtags, removing hashes and numbers, as well as segmenting out individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hash fix - will be called from within tweet cleaning function\n",
    "def hash_fix(h):\n",
    "    h1 = re.sub(r'[0-9]+', '', h)\n",
    "    h2 = re.sub(r'#', '', h1)\n",
    "    h3 = segment(str(h2))\n",
    "    h4 = ' '.join(map(str, h3)) \n",
    "    return h4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'uah discovery day'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_fix('#UAHDiscoveryDay2017')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary that contains each hash tag and the text it should be replaced with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs: dataframe with the tweets and the column with the hashtags\n",
    "def hash_dict(df,hash_col):\n",
    "    # Create a datafame of all hashtags in a column and their counts\n",
    "    # Note: hashtags are in lists inside a cell e.g. [#hash1, #hash2] \n",
    "    tag_counts = df[hash_col].apply(pd.Series).stack().value_counts().to_frame()\n",
    "    tag_counts = tag_counts.reset_index()\n",
    "    tag_counts.columns = ['hash','freq']\n",
    "    # Remove numbers and segment multiple words using hash fix\n",
    "    tag_counts = tag_counts.assign(clean_tag = tag_counts.hash.apply(lambda x: hash_fix(x)))\n",
    "    # Create a dictionary of the hashtags and their clean strings\n",
    "    tag_counts.set_index('hash', inplace=True)\n",
    "    tag_dict = tag_counts['clean_tag'].to_dict()\n",
    "    return tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'#chargernation': 'charger nation',\n",
       " '#uahuntsville': 'ua huntsville',\n",
       " '#uahopenhouse': 'uah open house',\n",
       " '#gochargers': 'go chargers',\n",
       " '#foundmyhome': 'found my home',\n",
       " '#chargeon': 'charge on',\n",
       " '#uah20': 'uah',\n",
       " '#uahorientation': 'uah orientation',\n",
       " '#awesome': 'awesome',\n",
       " '#uah17': 'uah',\n",
       " '#uah': 'uah',\n",
       " '#uahadmittedstudentday': 'uah admitted student day',\n",
       " '#uah18': 'uah',\n",
       " '#uahchargerpreview': 'uah charger preview',\n",
       " '#uahdiscoverydays': 'uah discovery days',\n",
       " '#uahroadtrip': 'uah road trip',\n",
       " '#uahtour': 'uah tour',\n",
       " '#uahdiscoveryday2017': 'uah discovery day',\n",
       " '#thisismyuah': 'this is my uah',\n",
       " '#chargerpride': 'charger pride',\n",
       " '#uah16': 'uah',\n",
       " '#uahnso': 'uah n so',\n",
       " '#uah19': 'uah',\n",
       " '#uah2022': 'uah',\n",
       " '#uah17facebook': 'uah facebook',\n",
       " '#uahbasketballbash': 'uah basketball bash',\n",
       " '#protip': 'pro tip',\n",
       " '#ff': 'ff',\n",
       " '#uahadmissions': 'uah admissions',\n",
       " '#foundmyhome16': 'found my home',\n",
       " '#universityofawesomeinhuntsville': 'university of awesome in huntsville',\n",
       " '#uahol': 'ua hol',\n",
       " '#myuah': 'my uah',\n",
       " '#uah21': 'uah',\n",
       " '#nomnoms': 'nom noms',\n",
       " '#puckapalooza': 'pucka palooza',\n",
       " '#uahvisit': 'uah visit',\n",
       " '#uah2016': 'uah',\n",
       " '#parentsessions': 'parent sessions',\n",
       " '#swag': 'swag',\n",
       " '#yourock': 'you rock',\n",
       " '#college': 'college',\n",
       " '#futurechargers': 'future chargers',\n",
       " '#followfriday': 'follow friday',\n",
       " '#uah22': 'uah',\n",
       " '#uahsummerevents': 'uah summer events',\n",
       " '#excited': 'excited',\n",
       " '#tbt': 'tbt',\n",
       " '#chargerpreview2018': 'charger preview',\n",
       " '#teamrockets': 'team rockets',\n",
       " '#ussrc': 'us src',\n",
       " '#destinationchargernation': 'destination charger nation',\n",
       " '#hunwx': 'hun wx',\n",
       " '#swagbag': 'swag bag',\n",
       " '#collegebound': 'collegebound',\n",
       " '#spooky': 'spooky',\n",
       " '#chargerblue': 'charger blue',\n",
       " '#win': 'win',\n",
       " '#pumped': 'pumped',\n",
       " '#blueschools': 'blue schools',\n",
       " '#huntsvillebound': 'huntsville bound',\n",
       " '#orientation': 'orientation',\n",
       " '#uahunstville': 'uah unst ville',\n",
       " '#gsc': 'gsc',\n",
       " '#battleofthebuffalo': 'battle of the buffalo',\n",
       " '#alwx': 'al wx',\n",
       " '#beautiful': 'beautiful',\n",
       " '#huntsville': 'huntsville',\n",
       " '#lunch': 'lunch',\n",
       " '#nom': 'nom',\n",
       " '#chargersat1': 'chargers at',\n",
       " '#uahchargers': 'uah chargers',\n",
       " '#parentsession': 'parent session',\n",
       " '#cwl': 'cwl',\n",
       " '#brightandearly': 'bright and early',\n",
       " '#welcome': 'welcome',\n",
       " '#runbabyrun': 'run baby run',\n",
       " '#high5friday': 'high friday',\n",
       " '#teambluebabies': 'team blue babies',\n",
       " '#o2santa': 'o santa',\n",
       " '#goblueandwhite': 'go blue and white',\n",
       " '#perfectweather': 'perfect weather',\n",
       " '#nature': 'nature',\n",
       " '#asduah': 'as du ah',\n",
       " '#alabama': 'alabama',\n",
       " '#sopretty': 'so pretty',\n",
       " '#winit': 'win it',\n",
       " '#uahsaidyes': 'uah said yes',\n",
       " '#ireallyshouldputachairontheroof': 'i really should put a chair on the roof',\n",
       " '#takeuahhomefortheholidays': 'take uah home for the holidays',\n",
       " '#winitweekend': 'win it weekend',\n",
       " '#teamusa': 'team usa',\n",
       " '#fcw': 'fcw',\n",
       " '#uahtastytuesday': 'uah tasty tuesday',\n",
       " '#rocketcity': 'rocket city',\n",
       " '#pumpyouup': 'pump you up',\n",
       " '#homecomingweek2011': 'homecoming week',\n",
       " '#proud': 'proud',\n",
       " '#gobigblue': 'go big blue',\n",
       " '#classof2013': 'class of',\n",
       " '#atx': 'atx',\n",
       " '#education': 'education',\n",
       " '#kyfbla': 'ky fbla',\n",
       " '#repost': 'repost',\n",
       " '#wilsonhall': 'wilson hall',\n",
       " '#nso': 'n so',\n",
       " '#iloveuah': 'i love uah',\n",
       " '#sunset': 'sunset',\n",
       " '#cbahotdogroast': 'cba hot dog roast',\n",
       " '#chargerbound': 'charger bound'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_dict = hash_dict(test_df,'hashtags')\n",
    "tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       College can be scary, but getting here doesn’t...\n",
       "1       We’re so excited to see everyone in the mornin...\n",
       "2       Online registration is open for our 3 Discover...\n",
       "3       If you’re interested in UAH and want to learn ...\n",
       "4       Yes! We will announce those dates a little clo...\n",
       "5       We will be hosting 3 overnight visits this sem...\n",
       "6       Interested in becoming a UAH Orientation Leade...\n",
       "7       Another record-breaking year! https://www.uah....\n",
       "8       Interested in UAH, but live too far away to vi...\n",
       "9       We are looking forward to having tons of stude...\n",
       "10      tbt Back when Charlie was just a high school s...\n",
       "11      Charger Preview is 2 weeks from Saturday! Don'...\n",
       "12      It's STILL not too late! https://twitter.com/U...\n",
       "13      We just love our Rocket City! Huntsville was r...\n",
       "14      Tomorrow is New Student Orientation Round ✋ Ge...\n",
       "15      Charger Preview is July 21! This event is for ...\n",
       "16      The University of Alabama in Huntsville will b...\n",
       "17      Tomorrow is our first Orientaion session of th...\n",
       "18      Our undergraduate students get to do some cool...\n",
       "19      For the third year in a row, UAH is ranked #1 ...\n",
       "20      Make sure you're following us on Snapchat (uah...\n",
       "21      UAH takes first place in the College/Universit...\n",
       "22      Register for Orientation TODAY! Sessions are f...\n",
       "23      It's finally Admitted Student Day! We are exci...\n",
       "24      Wonderful! Can't wait to see the pictures you ...\n",
       "25      The weather for Admitted Student Day tomorrow ...\n",
       "26      UAH has two teams in the competition this year...\n",
       "27      We can't wait for Admitted Student Day this Sa...\n",
       "28      Sign up for Orientation! New Student Orientati...\n",
       "29      Admitted Student Day is quickly approaching! A...\n",
       "                              ...                        \n",
       "1949    Don't forget to check out Diving for Dollars s...\n",
       "1950    Having a blast at #UAHuntsville WOW week? Send...\n",
       "1951    Move-in day is here!! Welcome to campus new Ch...\n",
       "1952    Move-in day is here!! Welcome to campus new Ch...\n",
       "1953    Kicking off the last Orientation! Welcome home...\n",
       "1954    The scholarship application will be closed for...\n",
       "1955    Scholarship applications for the 2011/2012 aca...\n",
       "1956    #UAHuntsville is up 18% in 2011 Freshmen admis...\n",
       "1957                           Move-in day is in 17 days!\n",
       "1958    RT @uahuntsville: R&B singer @jasonderulo will...\n",
       "1959    RT @uahuntsville: #UAHuntsville is now on Goog...\n",
       "1960    RT @uahuntsville: RT @SophieStrong: Im #Excite...\n",
       "1961    RT @waaytv: UAHuntsville Researchers to Improv...\n",
       "1962    RT @tomspacecamp: RT @SpaceCampSara: You can e...\n",
       "1963    Congrats to Rakeem Fuller for winning the $500...\n",
       "1964    RT @name_is_mucho: @UAHuntsville i got orienta...\n",
       "1965    RT @cweis1992: @UAHuntsville i am and can not ...\n",
       "1966    RT @haylibager: Orientation tomorrow @UAHuntsv...\n",
       "1967    RT @uahuntsville: Looking forward to having yo...\n",
       "1968    Good morning new Chargers! We are ready for or...\n",
       "1969    Orientation starts up again tomorrow! Welcome ...\n",
       "1970    Orientation 4 is underway! Welcome to the family!\n",
       "1971    Make sure to look for your button match while ...\n",
       "1972    Yes, we actually hold a record for the longest...\n",
       "1973    We haven't yet perfected time travel in scient...\n",
       "1974    You heard right, there is no charge or tuition...\n",
       "1975    New students at Orientation, we really think y...\n",
       "1976    Excited to see you all out for Orientation thi...\n",
       "1977    Don't forget to check in on Foursquare if you ...\n",
       "1978    Orientation Number 2 is underway! Remember to ...\n",
       "Name: tweet, Length: 1979, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.tweet.replace(tag_dict, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean tweet dataframe\n",
    "def clean_tweets(df, drop_cols):\n",
    "    # Create dictionary of \"cleaned\" hashtags\n",
    "    tag_dict = hash_dict(df,'hashtags')\n",
    "    # Create column with clean tweets\n",
    "    df = df.assign(clean_tw = df.tweet.apply(lambda x: pre.clean(str(x))))\n",
    "    # Replace hashtags in clean tweets using dictionary\n",
    "    df = df.assign(clean_tw = df.clean_tw.replace(tag_dict, regex=True))\n",
    "    # Drop unused cols\n",
    "    df = df.drop(drop_cols, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Clean DF\n",
    "# Columns to drop\n",
    "drop_cols = ['created_at', 'gif_thumb', 'is_quote_status', 'is_reply_to', \n",
    "    'location', 'mentions', 'name', 'place', 'quote_id', 'quote_url',\n",
    "    'replies', 'retweet', 'tags', 'time', 'timezone']\n",
    "new_df = clean_tweets(test_df, drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    College can be scary, but getting here doesn’t...\n",
       "1    We’re so excited to see everyone in the mornin...\n",
       "2    Online registration is open for our 3 Discover...\n",
       "3    If you’re interested in UAH and want to learn ...\n",
       "4    Yes! We will announce those dates a little clo...\n",
       "Name: clean_tw, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.clean_tw.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate Twitter Data from JSON Files into Single Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of JSON files\n",
    "twitter_files = glob.glob('../data/twitter_data/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.29\n"
     ]
    }
   ],
   "source": [
    "print(len(twitter_files)/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Having memory issues so I'm breaking the file list into chunks \n",
    "# If you have files you need to run individually, set no_tricks = False\n",
    "# and put your \"tricky\" file into line 9\n",
    "def appended_files(file_list, chunk_num, drop_cols, no_tricks):\n",
    "    appended_data = []\n",
    "    for file in file_list:\n",
    "        # Print file and time for debugging purposes\n",
    "        print(str(re.findall(r\"[0-9].*\", str(file))[0]) + ' - ' + str(datetime.datetime.now()))\n",
    "        tw_df = pd.read_json(file, encoding='utf-8', lines=True)\n",
    "        # Code is hanging on this large file - skip cleaning it\n",
    "        if (no_tricks == True and str(re.findall(r\"[0-9].*\", str(file))[0]) == '217156_main.json'):\n",
    "            continue\n",
    "        # Clean dataframe using function from previous step\n",
    "        tw_df = clean_tweets(tw_df, drop_cols)\n",
    "        # Extract ID number\n",
    "        id_num = ''.join([i for i in str(file) if i.isdigit()])\n",
    "        # Add ID number to DF\n",
    "        tw_df.loc[:,'ipeds_id']=id_num\n",
    "        # Add indicator for main page vs admissions page\n",
    "        if 'main' in str(file):\n",
    "            tw_df.loc[:,'main_page']=1\n",
    "        elif 'adm' in str(file):\n",
    "            tw_df.loc[:,'main_page']=0\n",
    "        else:\n",
    "            pass\n",
    "        # Reset index\n",
    "        tw_df.reset_index(drop=True, inplace=True)\n",
    "        # Add to list to append\n",
    "        if (len(file_list) > 1):\n",
    "            appended_data.append(tw_df)\n",
    "        # Run garbage collection\n",
    "        gc.collect()\n",
    "    # Concatenate files in list\n",
    "    if (len(file_list) > 1):\n",
    "        tw_df = pd.concat(appended_data, ignore_index=True)\n",
    "    # Save concatenated files to pickle\n",
    "    tw_df.to_pickle(path=r'../data/twitter_data/pickle/concat_tw_' + str(chunk_num) + '.pkl')\n",
    "    # Delete object\n",
    "    del tw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to create \"chunks\" of the list of files\n",
    "# https://stackoverflow.com/questions/434287/what-is-the-most-pythonic-way-to-iterate-over-a-list-in-chunks\n",
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to drop\n",
    "drop_cols = ['created_at', 'gif_thumb', 'is_quote_status', 'is_reply_to', \n",
    "    'location', 'mentions', 'name', 'place', 'quote_id', 'quote_url',\n",
    "    'replies', 'retweet', 'tags', 'time', 'timezone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15474"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216852_main.json - 2018-12-15 09:40:05.825548\n",
      "216931_adm.json - 2018-12-15 09:40:28.776106\n",
      "216931_main.json - 2018-12-15 09:40:30.202339\n",
      "217156_adm.json - 2018-12-15 09:40:38.210865\n",
      "217156_main.json - 2018-12-15 09:40:39.469350\n",
      "217165_adm.json - 2018-12-15 09:40:40.678950\n",
      "217165_main.json - 2018-12-15 09:40:52.651845\n",
      "217235_main.json - 2018-12-15 09:41:07.459032\n",
      "217420_adm.json - 2018-12-15 09:41:38.809539\n",
      "217420_main.json - 2018-12-15 09:41:41.815739\n",
      "217484_main.json - 2018-12-15 09:41:48.265548\n",
      "217493_main.json - 2018-12-15 09:41:48.668729\n",
      "217518_adm.json - 2018-12-15 09:41:54.018408\n",
      "217518_main.json - 2018-12-15 09:41:54.489165\n",
      "217536_main.json - 2018-12-15 09:42:10.210251\n",
      "217688_main.json - 2018-12-15 09:42:16.796390\n",
      "217749_main.json - 2018-12-15 09:42:30.173260\n",
      "217819_adm.json - 2018-12-15 09:42:33.692169\n",
      "217819_main.json - 2018-12-15 09:42:52.600567\n",
      "217864_main.json - 2018-12-15 09:43:31.124285\n",
      "217882_adm.json - 2018-12-15 09:43:37.932327\n",
      "217882_main.json - 2018-12-15 09:43:39.734309\n",
      "218061_main.json - 2018-12-15 09:43:50.672358\n",
      "218070_main.json - 2018-12-15 09:43:56.320167\n",
      "218229_main.json - 2018-12-15 09:44:21.422759\n",
      "218238_main.json - 2018-12-15 09:44:23.831659\n",
      "218663_adm.json - 2018-12-15 09:44:24.743224\n",
      "218663_main.json - 2018-12-15 09:44:29.319711\n",
      "218724_adm.json - 2018-12-15 09:45:06.466742\n",
      "218724_main.json - 2018-12-15 09:45:19.635414\n",
      "218733_main.json - 2018-12-15 09:45:32.519776\n",
      "218742_main.json - 2018-12-15 09:45:40.859098\n",
      "218964_main.json - 2018-12-15 09:46:10.300855\n",
      "218973_adm.json - 2018-12-15 09:46:17.633433\n",
      "218973_main.json - 2018-12-15 09:46:21.075775\n",
      "219000_main.json - 2018-12-15 09:46:22.884960\n",
      "219356_main.json - 2018-12-15 09:46:41.858735\n",
      "219471_adm.json - 2018-12-15 09:46:43.140073\n",
      "219471_main.json - 2018-12-15 09:46:43.949916\n",
      "219602_adm.json - 2018-12-15 09:46:46.399623\n",
      "219602_main.json - 2018-12-15 09:46:47.731211\n",
      "219709_main.json - 2018-12-15 09:46:53.823935\n",
      "219718_main.json - 2018-12-15 09:46:55.613410\n",
      "219806_main.json - 2018-12-15 09:46:58.828086\n",
      "219976_adm.json - 2018-12-15 09:47:01.089869\n",
      "219976_main.json - 2018-12-15 09:47:04.018030\n",
      "220075_adm.json - 2018-12-15 09:47:10.294415\n",
      "220075_main.json - 2018-12-15 09:47:11.553270\n",
      "220516_main.json - 2018-12-15 09:47:16.557988\n",
      "220613_main.json - 2018-12-15 09:47:20.564185\n",
      "220862_adm.json - 2018-12-15 09:47:30.263817\n",
      "220862_main.json - 2018-12-15 09:47:32.700185\n",
      "220978_adm.json - 2018-12-15 09:47:38.098459\n",
      "220978_main.json - 2018-12-15 09:47:39.230326\n",
      "221351_main.json - 2018-12-15 09:48:13.144293\n",
      "221740_main.json - 2018-12-15 09:48:19.625061\n",
      "221759_adm.json - 2018-12-15 09:48:41.536906\n",
      "221759_main.json - 2018-12-15 09:48:45.342488\n",
      "221768_adm.json - 2018-12-15 09:49:01.970390\n",
      "221768_main.json - 2018-12-15 09:49:05.706181\n",
      "221838_adm.json - 2018-12-15 09:49:07.107002\n",
      "221838_main.json - 2018-12-15 09:49:09.157963\n",
      "221847_adm.json - 2018-12-15 09:49:17.953691\n",
      "221847_main.json - 2018-12-15 09:49:21.085349\n",
      "221892_main.json - 2018-12-15 09:49:35.286218\n",
      "221953_main.json - 2018-12-15 09:49:46.631095\n",
      "221971_main.json - 2018-12-15 09:49:53.441247\n",
      "221999_main.json - 2018-12-15 09:49:57.513660\n",
      "222178_adm.json - 2018-12-15 09:50:52.088997\n",
      "222178_main.json - 2018-12-15 09:50:53.285300\n",
      "222831_adm.json - 2018-12-15 09:50:57.116234\n",
      "222831_main.json - 2018-12-15 09:50:58.183151\n",
      "223232_adm.json - 2018-12-15 09:51:01.418895\n",
      "223232_main.json - 2018-12-15 09:51:09.853514\n",
      "224004_adm.json - 2018-12-15 09:51:52.161166\n",
      "224004_main.json - 2018-12-15 09:51:55.415822\n",
      "224147_adm.json - 2018-12-15 09:52:09.558103\n",
      "224147_main.json - 2018-12-15 09:52:11.657230\n",
      "224226_adm.json - 2018-12-15 09:52:32.857693\n",
      "224226_main.json - 2018-12-15 09:52:34.273650\n",
      "224554_main.json - 2018-12-15 09:52:43.503674\n",
      "225247_main.json - 2018-12-15 09:52:50.032140\n",
      "225399_main.json - 2018-12-15 09:52:56.771453\n",
      "225432_adm.json - 2018-12-15 09:53:03.197551\n",
      "225432_main.json - 2018-12-15 09:53:03.981996\n",
      "225627_adm.json - 2018-12-15 09:53:12.191919\n",
      "225627_main.json - 2018-12-15 09:53:14.612136\n",
      "226091_main.json - 2018-12-15 09:53:27.223761\n",
      "226231_main.json - 2018-12-15 09:53:33.381082\n",
      "226471_main.json - 2018-12-15 09:53:38.001829\n",
      "226833_adm.json - 2018-12-15 09:53:41.013412\n",
      "226833_main.json - 2018-12-15 09:53:45.205185\n",
      "227331_main.json - 2018-12-15 09:54:00.387464\n",
      "227368_main.json - 2018-12-15 09:54:03.518559\n",
      "227526_adm.json - 2018-12-15 09:54:12.517065\n",
      "227526_main.json - 2018-12-15 09:54:14.796946\n",
      "227757_main.json - 2018-12-15 09:54:22.982959\n",
      "227881_main.json - 2018-12-15 09:55:20.961040\n",
      "228149_adm.json - 2018-12-15 09:55:43.484404\n",
      "228149_main.json - 2018-12-15 09:55:45.954028\n",
      "228246_main.json - 2018-12-15 09:56:31.383441\n",
      "228343_main.json - 2018-12-15 09:56:58.264767\n",
      "228431_main.json - 2018-12-15 09:57:24.509846\n",
      "228459_adm.json - 2018-12-15 09:57:29.051135\n",
      "228459_main.json - 2018-12-15 09:57:30.988475\n",
      "228529_main.json - 2018-12-15 09:59:11.307468\n",
      "228705_adm.json - 2018-12-15 09:59:16.448504\n",
      "228705_main.json - 2018-12-15 09:59:19.852077\n",
      "228723_adm.json - 2018-12-15 09:59:30.304444\n",
      "228723_main.json - 2018-12-15 09:59:36.821327\n",
      "228778_adm.json - 2018-12-15 10:00:26.844340\n",
      "228778_main.json - 2018-12-15 10:00:29.671582\n",
      "228787_main.json - 2018-12-15 10:01:10.273169\n",
      "228802_adm.json - 2018-12-15 10:01:36.549744\n",
      "228802_main.json - 2018-12-15 10:01:44.611273\n",
      "228875_adm.json - 2018-12-15 10:01:49.842294\n",
      "228875_main.json - 2018-12-15 10:01:51.309125\n",
      "229063_adm.json - 2018-12-15 10:02:03.103598\n",
      "229063_main.json - 2018-12-15 10:02:03.685216\n",
      "229115_adm.json - 2018-12-15 10:02:05.985510\n",
      "229115_main.json - 2018-12-15 10:02:13.788750\n",
      "229179_main.json - 2018-12-15 10:03:08.847627\n",
      "229267_adm.json - 2018-12-15 10:03:12.161921\n",
      "229267_main.json - 2018-12-15 10:03:23.928212\n",
      "229780_main.json - 2018-12-15 10:03:45.723907\n",
      "229814_main.json - 2018-12-15 10:03:47.731581\n",
      "230038_adm.json - 2018-12-15 10:03:59.269025\n",
      "230038_main.json - 2018-12-15 10:03:59.887932\n",
      "230603_main.json - 2018-12-15 10:04:07.191657\n",
      "230728_main.json - 2018-12-15 10:04:16.713584\n",
      "230737_main.json - 2018-12-15 10:04:25.890197\n",
      "230764_adm.json - 2018-12-15 10:04:28.952842\n",
      "230764_main.json - 2018-12-15 10:04:30.145091\n",
      "230782_main.json - 2018-12-15 10:05:49.270622\n",
      "230807_main.json - 2018-12-15 10:05:54.431944\n",
      "230995_main.json - 2018-12-15 10:06:11.783807\n",
      "231174_main.json - 2018-12-15 10:06:19.241286\n",
      "231624_adm.json - 2018-12-15 10:06:33.534370\n",
      "231624_main.json - 2018-12-15 10:06:35.593883\n",
      "231712_adm.json - 2018-12-15 10:06:54.796048\n",
      "231712_main.json - 2018-12-15 10:06:56.774414\n",
      "232423_main.json - 2018-12-15 10:06:59.819222\n",
      "232557_adm.json - 2018-12-15 10:07:21.564342\n",
      "232557_main.json - 2018-12-15 10:07:30.499124\n",
      "232566_adm.json - 2018-12-15 10:07:48.340644\n",
      "232566_main.json - 2018-12-15 10:07:56.968301\n",
      "232609_adm.json - 2018-12-15 10:08:19.601780\n",
      "232609_main.json - 2018-12-15 10:08:20.996833\n",
      "232681_main.json - 2018-12-15 10:08:39.745853\n",
      "232706_adm.json - 2018-12-15 10:08:45.316772\n",
      "232706_main.json - 2018-12-15 10:08:51.703668\n",
      "232982_main.json - 2018-12-15 10:08:55.286009\n",
      "233277_adm.json - 2018-12-15 10:09:00.126319\n",
      "233277_main.json - 2018-12-15 10:09:01.126804\n",
      "233374_adm.json - 2018-12-15 10:09:06.858417\n",
      "233374_main.json - 2018-12-15 10:09:09.269080\n",
      "233921_adm.json - 2018-12-15 10:09:26.192664\n",
      "233921_main.json - 2018-12-15 10:09:28.882497\n",
      "234030_adm.json - 2018-12-15 10:09:48.818476\n",
      "234030_main.json - 2018-12-15 10:09:50.941061\n",
      "234076_adm.json - 2018-12-15 10:10:31.291687\n",
      "234076_main.json - 2018-12-15 10:10:31.714912\n",
      "234155_adm.json - 2018-12-15 10:11:09.647369\n",
      "234155_main.json - 2018-12-15 10:11:10.181678\n",
      "234207_adm.json - 2018-12-15 10:11:20.440426\n",
      "234207_main.json - 2018-12-15 10:11:22.349236\n",
      "234827_main.json - 2018-12-15 10:11:35.709595\n",
      "235097_main.json - 2018-12-15 10:11:41.472088\n",
      "235167_main.json - 2018-12-15 10:11:45.552659\n",
      "235316_adm.json - 2018-12-15 10:11:48.158019\n",
      "235316_main.json - 2018-12-15 10:11:49.516514\n",
      "236230_adm.json - 2018-12-15 10:12:01.498055\n",
      "236230_main.json - 2018-12-15 10:12:03.746305\n",
      "236328_main.json - 2018-12-15 10:12:13.659014\n",
      "236577_main.json - 2018-12-15 10:12:50.127751\n",
      "236595_adm.json - 2018-12-15 10:13:06.267144\n",
      "236595_main.json - 2018-12-15 10:13:07.227192\n",
      "236948_adm.json - 2018-12-15 10:13:16.941654\n",
      "236948_main.json - 2018-12-15 10:13:17.909074\n",
      "237011_main.json - 2018-12-15 10:13:53.188303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237057_main.json - 2018-12-15 10:14:38.634305\n",
      "237066_main.json - 2018-12-15 10:14:43.997505\n",
      "237330_main.json - 2018-12-15 10:14:48.558478\n",
      "237367_main.json - 2018-12-15 10:14:50.770827\n",
      "237525_adm.json - 2018-12-15 10:15:01.573148\n",
      "237525_main.json - 2018-12-15 10:15:03.818168\n",
      "237792_main.json - 2018-12-15 10:15:12.842286\n",
      "237899_main.json - 2018-12-15 10:15:16.160757\n",
      "237932_main.json - 2018-12-15 10:15:19.255441\n",
      "238193_main.json - 2018-12-15 10:15:24.732266\n",
      "238430_adm.json - 2018-12-15 10:15:27.750687\n",
      "238430_main.json - 2018-12-15 10:15:28.744925\n",
      "238458_main.json - 2018-12-15 10:15:33.303112\n",
      "238476_main.json - 2018-12-15 10:15:46.258073\n",
      "238616_main.json - 2018-12-15 10:15:51.865389\n",
      "238980_main.json - 2018-12-15 10:16:02.273826\n",
      "239080_main.json - 2018-12-15 10:16:07.817503\n",
      "239105_adm.json - 2018-12-15 10:16:15.112221\n",
      "239105_main.json - 2018-12-15 10:16:22.676174\n",
      "239318_main.json - 2018-12-15 10:17:51.789181\n",
      "240107_main.json - 2018-12-15 10:17:59.693522\n",
      "240189_adm.json - 2018-12-15 10:18:18.476516\n",
      "240189_main.json - 2018-12-15 10:18:24.699190\n",
      "240268_adm.json - 2018-12-15 10:18:29.324248\n",
      "240268_main.json - 2018-12-15 10:18:33.068975\n",
      "240277_adm.json - 2018-12-15 10:18:50.139031\n",
      "240277_main.json - 2018-12-15 10:18:52.282026\n",
      "240329_adm.json - 2018-12-15 10:19:18.694392\n",
      "240329_main.json - 2018-12-15 10:19:19.252992\n",
      "240365_adm.json - 2018-12-15 10:19:21.954444\n",
      "240365_main.json - 2018-12-15 10:19:29.001280\n",
      "240374_adm.json - 2018-12-15 10:19:48.420129\n",
      "240374_main.json - 2018-12-15 10:19:51.099373\n",
      "240417_adm.json - 2018-12-15 10:19:54.878363\n",
      "240417_main.json - 2018-12-15 10:19:57.579096\n",
      "240444_adm.json - 2018-12-15 10:20:01.222918\n",
      "240444_main.json - 2018-12-15 10:20:04.818923\n",
      "240462_main.json - 2018-12-15 10:23:38.100349\n",
      "240471_main.json - 2018-12-15 10:23:49.683924\n",
      "240480_main.json - 2018-12-15 10:23:58.358429\n",
      "240727_main.json - 2018-12-15 10:24:03.396915\n",
      "243744_adm.json - 2018-12-15 10:24:07.490239\n",
      "243744_main.json - 2018-12-15 10:24:08.359617\n",
      "243780_adm.json - 2018-12-15 10:24:45.493916\n",
      "243780_main.json - 2018-12-15 10:24:48.281308\n",
      "366711_main.json - 2018-12-15 10:26:19.436469\n",
      "450933_adm.json - 2018-12-15 10:26:26.106177\n",
      "450933_main.json - 2018-12-15 10:26:32.805968\n"
     ]
    }
   ],
   "source": [
    "# Clean and append files in chunks\n",
    "# Note: Computer hanging up - do this in chunks\n",
    "# If process hangs and you need to restart at a chunk, change twitter files to \n",
    "# twitter_files[start_file_number:] and i to the number of the chunk\n",
    "# Example: i=5 twitter_files[250:] will start at the 5th chunk for chunks of 50\n",
    "i = 1\n",
    "for group in chunker(twitter_files, 50):\n",
    "    chunk_str = \"%02d\" % (i,)\n",
    "    appended_files(group, chunk_str, drop_cols, no_tricks=True)\n",
    "    # Run garbage collection\n",
    "    gc.collect()\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217156_fixed_main.json - 2018-12-17 19:33:11.123541\n"
     ]
    }
   ],
   "source": [
    "# For some reason one .json file will not process. Tried stripping non-ascii characters.\n",
    "# It will have to be left out for now. \n",
    "# Run one large, pesky file separately - 217156_main.json\n",
    "#appended_files(['../data/twitter_data/217156_fixed_main.json'], '217156', drop_cols, no_tricks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/twitter_data/pickle\\concat_tw_01.pkl\n",
      "../data/twitter_data/pickle\\concat_tw_02.pkl\n",
      "../data/twitter_data/pickle\\concat_tw_03.pkl\n",
      "../data/twitter_data/pickle\\concat_tw_04.pkl\n",
      "../data/twitter_data/pickle\\concat_tw_05.pkl\n",
      "../data/twitter_data/pickle\\concat_tw_06.pkl\n",
      "../data/twitter_data/pickle\\concat_tw_07.pkl\n",
      "../data/twitter_data/pickle\\concat_tw_08.pkl\n",
      "../data/twitter_data/pickle\\concat_tw_09.pkl\n",
      "../data/twitter_data/pickle\\concat_tw_10.pkl\n",
      "../data/twitter_data/pickle\\concat_tw_11.pkl\n",
      "../data/twitter_data/pickle\\concat_tw_12.pkl\n",
      "../data/twitter_data/pickle\\concat_tw_13.pkl\n"
     ]
    }
   ],
   "source": [
    "# Concatenate only the cleaned tweets from the cleaned pkl files\n",
    "# Saves memory space\n",
    "gc.collect()\n",
    "pkl_files = glob.glob(r'../data/twitter_data/pickle/concat_tw_*.pkl')\n",
    "append_tweets = []\n",
    "for file in pkl_files:\n",
    "    print(str(file))\n",
    "    tw_df = pd.read_pickle(file)\n",
    "    tw_df = tw_df[['ipeds_id', 'id', 'clean_tw']]\n",
    "    append_tweets.append(tw_df)\n",
    "    del tw_df\n",
    "    gc.collect()\n",
    "tw_final = pd.concat(append_tweets, ignore_index=True)\n",
    "tw_final.to_pickle(path=r'../data/clean/clean_tweets_full.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Text and Create Diversity Tweet Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pickled data\n",
    "tw_data = pd.read_pickle(path=r'../data/clean/clean_tweets_full.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately there are still non-ascii characters - let's remove them now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_unicode(text):\n",
    "    return unicodedata2.normalize('NFKD', text).encode('ascii', 'ignore')\n",
    "\n",
    "tw_data = tw_data.assign(clean_tw = tw_data.clean_tw.apply(convert_unicode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing code comes from link below. Though, note the author forgot to define stemmer. \n",
    "\n",
    "https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem and remove stop words\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_data = tw_data.assign(words = tw_data['clean_tw'].map(preprocess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a copy in case you need to reload\n",
    "tw_data.to_pickle(path=r'../data/clean/pre_processed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_data = pd.read_pickle(path=r'../data/clean/pre_processed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a copy of the data with only the processed data, id, and school id\n",
    "tw_words = tw_data[['ipeds_id', 'id', 'words']].copy()\n",
    "del tw_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)\n",
    "# Create a column with random number - will be used to subset the data\n",
    "tw_words = tw_words.assign(rand_int = np.random.randint(0, 99, tw_words.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove words used by only one college"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove words used by only one school - this will help to filter out school names and mascots\n",
    "# This is tricky again because of memory issues\n",
    "# Stack the words in chunks - remove words that are used by multiple schools\n",
    "# Final result will be list of words used only by one school\n",
    "\n",
    "# Must have list of words in tweet in column called 'words'\n",
    "def single_sch_words(df):\n",
    "\n",
    "    # Stack the data - create row for each word in list\n",
    "    df = df.set_index(['id'])\n",
    "    df.sort_index(inplace=True)\n",
    "    s = df.apply(lambda x: pd.Series(x['words']),axis=1).stack().reset_index(level=1, drop=True)\n",
    "    s.name = 'word'\n",
    "    tw_data_stacked = df.drop('words', axis=1).join(s)\n",
    "    del s\n",
    "    gc.collect()\n",
    "    \n",
    "    # Keep only unique instances of each word and school\n",
    "    tw_data_stacked = tw_data_stacked.drop_duplicates(subset=['word', 'ipeds_id'], keep=False)\n",
    "    \n",
    "    # Create count of number of colleges that use each word\n",
    "    num_colleges = tw_data_stacked.groupby('word').agg({'ipeds_id': lambda x: x.nunique()})\n",
    "    num_colleges.name = 'word'\n",
    "    num_colleges.reset_index()\n",
    "    num_colleges.rename(index=str, columns={'ipeds_id':'school_count'}, inplace=True)\n",
    "    tw_data_stacked = tw_data_stacked.join(num_colleges,on='word')\n",
    "    del num_colleges\n",
    "    gc.collect()\n",
    "    \n",
    "    # Keep words used by only one college\n",
    "    tw_data_stacked.reset_index()\n",
    "    tw_data_stacked = tw_data_stacked.loc[tw_data_stacked.school_count == 1]\n",
    "    tw_data_stacked.drop('school_count', axis=1, inplace=True)\n",
    "    gc.collect()\n",
    "    \n",
    "    return tw_data_stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ipeds_id</th>\n",
       "      <th>rand_int</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93773412</th>\n",
       "      <td>183239</td>\n",
       "      <td>0</td>\n",
       "      <td>orvieto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789579480</th>\n",
       "      <td>210669</td>\n",
       "      <td>0</td>\n",
       "      <td>ascent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836384863</th>\n",
       "      <td>209056</td>\n",
       "      <td>0</td>\n",
       "      <td>mcdow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908199047</th>\n",
       "      <td>164580</td>\n",
       "      <td>0</td>\n",
       "      <td>nextworth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922205934</th>\n",
       "      <td>228875</td>\n",
       "      <td>0</td>\n",
       "      <td>buccan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ipeds_id  rand_int       word\n",
       "id                                     \n",
       "93773412    183239         0    orvieto\n",
       "789579480   210669         0     ascent\n",
       "836384863   209056         0      mcdow\n",
       "908199047   164580         0  nextworth\n",
       "922205934   228875         0     buccan"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Datasets of single use words for random chunks of the data\n",
    "# Example using 1% of data\n",
    "single_words1 = single_sch_words(tw_words.loc[tw_words.rand_int < 1])\n",
    "single_words1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get single words from random samples of 3% of data at a time then append\n",
    "append_single = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start at: 0 End at: 2 - 2018-12-18 13:30:07.326287\n",
      "Start at: 3 End at: 5 - 2018-12-18 13:31:59.735666\n",
      "Start at: 6 End at: 8 - 2018-12-18 13:33:41.026483\n",
      "Start at: 9 End at: 11 - 2018-12-18 13:35:33.474505\n",
      "Start at: 12 End at: 14 - 2018-12-18 13:37:22.630545\n",
      "Start at: 15 End at: 17 - 2018-12-18 13:39:05.125096\n",
      "Start at: 18 End at: 20 - 2018-12-18 13:40:41.995758\n",
      "Start at: 21 End at: 23 - 2018-12-18 13:42:21.380203\n",
      "Start at: 24 End at: 26 - 2018-12-18 13:43:59.934431\n",
      "Start at: 27 End at: 29 - 2018-12-18 13:45:41.805603\n",
      "Start at: 30 End at: 32 - 2018-12-18 13:47:18.330342\n",
      "Start at: 33 End at: 35 - 2018-12-18 13:48:58.463605\n",
      "Start at: 36 End at: 38 - 2018-12-18 13:50:38.119495\n",
      "Start at: 39 End at: 41 - 2018-12-18 13:52:28.329703\n",
      "Start at: 42 End at: 44 - 2018-12-18 13:54:18.916873\n",
      "Start at: 45 End at: 47 - 2018-12-18 13:56:03.736739\n",
      "Start at: 48 End at: 50 - 2018-12-18 13:57:48.435560\n",
      "Start at: 51 End at: 53 - 2018-12-18 13:59:37.243402\n",
      "Start at: 54 End at: 56 - 2018-12-18 14:01:30.655424\n",
      "Start at: 57 End at: 59 - 2018-12-18 14:03:15.413516\n",
      "Start at: 60 End at: 62 - 2018-12-18 14:05:07.227998\n",
      "Start at: 63 End at: 65 - 2018-12-18 14:06:52.656103\n",
      "Start at: 66 End at: 68 - 2018-12-18 14:08:40.924504\n",
      "Start at: 69 End at: 71 - 2018-12-18 14:10:27.186727\n",
      "Start at: 72 End at: 74 - 2018-12-18 14:12:03.697649\n",
      "Start at: 75 End at: 77 - 2018-12-18 14:13:35.829157\n",
      "Start at: 78 End at: 80 - 2018-12-18 14:15:11.689331\n",
      "Start at: 81 End at: 83 - 2018-12-18 14:17:02.079004\n",
      "Start at: 84 End at: 86 - 2018-12-18 14:18:34.542096\n",
      "Start at: 87 End at: 89 - 2018-12-18 14:20:26.194343\n",
      "Start at: 90 End at: 92 - 2018-12-18 14:22:11.956262\n",
      "Start at: 93 End at: 95 - 2018-12-18 14:23:54.153736\n",
      "Start at: 96 End at: 98 - 2018-12-18 14:25:38.045927\n",
      "Start at: 99 End at: 101 - 2018-12-18 14:27:21.715944\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,102,3):\n",
    "    j = i + 2\n",
    "    print('Start at: ' + str(i) + ' End at: ' + str(j) + ' - ' + str(datetime.datetime.now())) \n",
    "    df = single_sch_words(tw_words.loc[(tw_words.rand_int >= i) & (tw_words.rand_int <= j)])\n",
    "    append_single.append(df)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the three sets of single words, see if any are repeats\n",
    "single_words_all = pd.concat(append_single, ignore_index=True)\n",
    "# Save in case of crash\n",
    "single_words_all.to_pickle(path=r'../data/clean/single_words.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_words_all = pd.read_pickle(path=r'../data/clean/single_words.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ipeds_id</th>\n",
       "      <th>rand_int</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>152080</td>\n",
       "      <td>1</td>\n",
       "      <td>markup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>164580</td>\n",
       "      <td>2</td>\n",
       "      <td>zachrarki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>209056</td>\n",
       "      <td>0</td>\n",
       "      <td>mcdow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>210669</td>\n",
       "      <td>2</td>\n",
       "      <td>shemelya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>218742</td>\n",
       "      <td>1</td>\n",
       "      <td>dsc_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>173300</td>\n",
       "      <td>2</td>\n",
       "      <td>gooseberri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>209056</td>\n",
       "      <td>2</td>\n",
       "      <td>rouari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>218742</td>\n",
       "      <td>2</td>\n",
       "      <td>liisa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>218742</td>\n",
       "      <td>2</td>\n",
       "      <td>salosaari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>218742</td>\n",
       "      <td>2</td>\n",
       "      <td>jasinski</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ipeds_id  rand_int        word\n",
       "0   152080         1      markup\n",
       "1   164580         2   zachrarki\n",
       "2   209056         0       mcdow\n",
       "3   210669         2    shemelya\n",
       "4   218742         1        dsc_\n",
       "5   173300         2  gooseberri\n",
       "6   209056         2      rouari\n",
       "7   218742         2       liisa\n",
       "8   218742         2   salosaari\n",
       "9   218742         2    jasinski"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On further inspection, many of the single words look like mentions that weren't removed with the Twitter preprocessor.\n",
    "single_words_all.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On the combined dataset of random samples, count the number of schools that use each word\n",
    "num_colleges = single_words_all.groupby('word').agg({'ipeds_id': lambda x: x.nunique()})\n",
    "num_colleges.name = 'word'\n",
    "num_colleges.reset_index()\n",
    "num_colleges.rename(index=str, columns={'ipeds_id':'school_count'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_words_all = num_colleges\n",
    "single_words_all = single_words_all.loc[single_words_all.school_count == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a__schedul</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a__www</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_clever_alia</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_dzi</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_feiii</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_garza_duh</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_n_g</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_scot_</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_suarez</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_wizzzl</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               school_count\n",
       "word                       \n",
       "a__schedul                1\n",
       "a__www                    1\n",
       "a_clever_alia             1\n",
       "a_dzi                     1\n",
       "a_feiii                   1\n",
       "a_garza_duh               1\n",
       "a_n_g                     1\n",
       "a_scot_                   1\n",
       "a_suarez                  1\n",
       "a_wizzzl                  1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_words_all.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['word', 'school_count'], dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_words_all = single_words_all.reset_index()\n",
    "single_words_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_word_list = single_words_all.word.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a__schedul',\n",
       " 'a__www',\n",
       " 'a_clever_alia',\n",
       " 'a_dzi',\n",
       " 'a_feiii',\n",
       " 'a_garza_duh',\n",
       " 'a_n_g',\n",
       " 'a_scot_',\n",
       " 'a_suarez',\n",
       " 'a_wizzzl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_word_list[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove words used by only one college from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_words.drop(['rand_int'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_single(old_list, not_list):\n",
    "    return [x for x in old_list if x not in not_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-a4786b178c97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtw_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtw_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtw_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mremove_single\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msingle_word_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3192\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3193\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3194\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3196\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/src\\inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-a4786b178c97>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtw_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtw_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtw_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mremove_single\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msingle_word_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-27-eb913ead9e07>\u001b[0m in \u001b[0;36mremove_single\u001b[1;34m(old_list, not_list)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mremove_single\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mold_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnot_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mold_list\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnot_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-27-eb913ead9e07>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mremove_single\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mold_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnot_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mold_list\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnot_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This takes a very very very long time to run\n",
    "# Did not have time to finish running, sadly\n",
    "tw_words = tw_words.assign(words = tw_words.words.apply(lambda x: remove_single(x, single_word_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_words.to_pickle(path=r'../data/clean/clean_tweets_final.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diversity Analysis Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw_data = pd.read_pickle(path=r'../data/clean/pre_processed.pkl')\n",
    "tw_words = tw_data[['ipeds_id', 'id', 'words']].copy()\n",
    "del tw_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1053360850965315584</td>\n",
       "      <td>[uabhomecom, favorit, time, year]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1052568776414298117</td>\n",
       "      <td>[admit, incom, freshman, class, congrat, check...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1021825776553996288</td>\n",
       "      <td>[growth, record, enrol, build, transit, commut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1016333689486233600</td>\n",
       "      <td>[hope, have, great, summer, admit, freshmen, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1006548327687745536</td>\n",
       "      <td>[recent, admit, freshman, congrat, help, check...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                              words\n",
       "0  1053360850965315584                  [uabhomecom, favorit, time, year]\n",
       "1  1052568776414298117  [admit, incom, freshman, class, congrat, check...\n",
       "2  1021825776553996288  [growth, record, enrol, build, transit, commut...\n",
       "3  1016333689486233600  [hope, have, great, summer, admit, freshmen, t...\n",
       "4  1006548327687745536  [recent, admit, freshman, congrat, help, check..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw_words.drop(['ipeds_id'], axis=1, inplace=True)\n",
    "tw_words.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/twitter_data/pickle\\concat_tw_01.pkl\n",
      "../data/twitter_data/pickle\\concat_tw_02.pkl\n",
      "../data/twitter_data/pickle\\concat_tw_03.pkl\n",
      "../data/twitter_data/pickle\\concat_tw_04.pkl\n",
      "../data/twitter_data/pickle\\concat_tw_05.pkl\n",
      "../data/twitter_data/pickle\\concat_tw_06.pkl\n",
      "../data/twitter_data/pickle\\concat_tw_07.pkl\n",
      "../data/twitter_data/pickle\\concat_tw_08.pkl\n",
      "../data/twitter_data/pickle\\concat_tw_09.pkl\n",
      "../data/twitter_data/pickle\\concat_tw_10.pkl\n",
      "../data/twitter_data/pickle\\concat_tw_11.pkl\n",
      "../data/twitter_data/pickle\\concat_tw_12.pkl\n",
      "../data/twitter_data/pickle\\concat_tw_13.pkl\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "pkl_files = glob.glob(r'../data/twitter_data/pickle/concat_tw_*.pkl')\n",
    "append_tweets = []\n",
    "for file in pkl_files:\n",
    "    print(str(file))\n",
    "    tw_df = pd.read_pickle(file)\n",
    "    tw_df = tw_df[['ipeds_id', 'id', 'date','tweet','likes_count','photos']]\n",
    "    append_tweets.append(tw_df)\n",
    "    del tw_df\n",
    "    gc.collect()\n",
    "diversity_df = pd.concat(append_tweets, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "diversity_df = pd.merge(diversity_df, tw_words, on=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del tw_words\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_contains(cell_list, word_list):\n",
    "    if [i for i in cell_list if i in word_list]:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(list_contains(['1','2','3'], ['3']))\n",
    "print(list_contains(['1','2','3'], ['4']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify diversity-related tweets\n",
    "div_words = ['divers', 'multicultur']\n",
    "diversity_df = diversity_df.assign(diversity_flag = diversity_df.words.apply(lambda x: list_contains(x,div_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify race-related tweets\n",
    "race_words = ['black', 'african', 'asian', 'hispan', 'latino', 'latina']\n",
    "diversity_df = diversity_df.assign(race_flag = diversity_df.words.apply(lambda x: list_contains(x,race_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify gender-related tweets\n",
    "gender_words = ['woman', 'women','gender']\n",
    "diversity_df = diversity_df.assign(gender_flag = diversity_df.words.apply(lambda x: list_contains(x,gender_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "425    The 2013 Fall Leadership Conference is underwa...\n",
       "528    Happy Memorial Day, friends! Thank you to all ...\n",
       "874    RT @uabnews: They did it! The UAB Women's Bask...\n",
       "878    Women's Basketball Invitational Championship G...\n",
       "933    2 new women's sports at UAB!!!  http://fb.me/x...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diversity_df[diversity_df.gender_flag==True].tweet.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ipeds_id', 'id', 'date', 'tweet', 'likes_count', 'photos', 'words',\n",
       "       'diversity_flag', 'race_flag', 'gender_flag'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diversity_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5907245"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(diversity_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data in chunks\n",
    "diversity_df[0:1000000].to_pickle(path=r'../data/clean/diversity_full_01.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "diversity_df[1000001:2000000].to_pickle(path=r'../data/clean/diversity_full_02.pkl')\n",
    "gc.collect()\n",
    "diversity_df[2000001:3000000].to_pickle(path=r'../data/clean/diversity_full_03.pkl')\n",
    "gc.collect()\n",
    "diversity_df[3000001:4000000].to_pickle(path=r'../data/clean/diversity_full_04.pkl')\n",
    "gc.collect()\n",
    "diversity_df[4000001:5000000].to_pickle(path=r'../data/clean/diversity_full_05.pkl')\n",
    "gc.collect()\n",
    "diversity_df[5000001:].to_pickle(path=r'../data/clean/diversity_full_06.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
